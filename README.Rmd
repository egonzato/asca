---
title: "asca"
author: "Elia Gonzato"
date: "2023-08-20"
output: 
  md_document:
    variant: markdown_github
---

## Introduction

The aim of this repository is to provide an implementation, in R, of the permutation test for ASCA, which has been proposed recently in [literature](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-322).

## Anova Simultaneous Component Analysis

### Motivation 
The need for ASCA comes from the fact that ANOVA is a univariate method that takes into account that there is an underlying design of the experiment in the data of interest, but it does not capture the relationship between the variables that are collected. The opposite is valid for other multivariate models, such as PCA and SCA, which can capture the relationship between variables but do not take into account that there is an experimental design at the source
of information.
As these two models only give a limited view, we need a model that takes
into account both aspects of the experiment. 

ASCA has been developed exactly for this kind of task and aims to solve the previously explained problem by merging the estimation aspect of ANOVA with PCA so that drawbacks of the other methods are not highlighted.

### Permutation test

ASCA is also thought of as a tool for parameters estimation, just like ANOVA, which means that we can also test values that are estimated in the rest part of the process. Unfortunately there is no theoretical distribution that can be used to compare estimated and tabulated values; the most appropriate solution, published lately, is the permutation test, which means that columns of our original dataset are used, a test statistic is obtained and this process is repeated until a theoretical distribution is derived. Percentiles of this theoretical distribution are then used to compare the permuted statistic to the observed one, to infer about the significance of the parameter.

The test statistic of interest is the following:

$$F=\frac{\frac{SSE_{red}-SSE_{full}}{dfe_{red}-dfe_{full}}}{\frac{SSE_{full}}{dfe_{full}}}$$

## Example

The following examples will use a simulated dataset, which is obtained from the *simulate.R* script.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(ggplot2)
library(multiblock)
library(stringr)
library(dplyr)
library(gtools)
# simulate four categorical variables
# set obs for each var
n=300
# create sort of doe matrix
set.seed(181299)
h=sample(rep(seq(1,3,1),100))
w=sample(rep(seq(1,5,1),60))
z=sample(rep(seq(1,6,1),50))
doe=as.data.frame(cbind(h,w,z))
doe=as.data.frame(lapply(doe,as.factor))
# now suppose we are in the omics context, let's simulate 100 y variables
ys=c()
# I will concatenate all 100 variables to the 
# same vector and then reorder it in a matrix
for (j in 1:100){
  set.seed(181299+j*10)
  mu=round(ceiling(runif(1,15,25)))
  sigma=round(ceiling(runif(1,0,5)))
  y=abs(rnorm(n,mu,sigma))
  ys=c(ys,y)
}
# add factors to the same matrix, asIs fundamental to keep ys in the same col
ymat=matrix(ys,ncol=100)
baba=data.frame(Y=I(matrix(ys,ncol=100)),doe)
```

Once the dataset has been simulated, the model can be computed and the variance explained by each variable can be plotted:

```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width='70%', fig.align='center'}
model=asca(data=baba,
           Y ~ .,
           pca.in = 5)
# plot for explained variance
explained=function(model=NULL){
  bp=data.frame(model$explvar*100,str_to_title(names(model$explvar)))
  bp$str_to_title.names.model.explvar..[4]='Residual'
  names(bp)=c('Variance','Name')
  ggplot(aes(x=reorder(Name,Variance,decreasing=T),
             y=Variance,
             fill=Variance),
         data=bp)+
    geom_bar(stat="identity", colour='black')+
    scale_fill_gradient2(low="orange", high="red") +
    labs(x="\nDesign Effects", y="Explained variance\n", title = 'Variance explained by each variable') +
    theme_bw()
  
}
explained(model)

```

As we can see, there is no real effect on the dependent variable, this happens because values are simulated randomly and categorical features are not helpful in explaining variability.

We can also plot projections of observations into the latent space defined by principal component. In particular for the second variable:

```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width='70%', fig.align='center'}
# function where k stands for the index of the considered variable
plotscoreasca=function(model=NULL,k=NULL){
  # select scores
  projected=model[[3]]
  proj.obs=as.data.frame(projected[k])[,1:2]
  scores=as.data.frame(model[[1]][k])[,1:2]
  # select var to be used for testing
  var.col=doe[[k]]
  # cbind comp var
  df.plot=as.data.frame(cbind(proj.obs,var.col,scores))
  names(df.plot)=c('P1','P2','Group','S1','S2')
  # plot
  df.plot %>%
    ggplot()+
    geom_point(aes(x=P1,y=P2,color=Group))+
    geom_point(aes(x=S1,y=S2,color=Group,size=1.5))+
    stat_ellipse(aes(x=P1,y=P2,color=Group))+
    guides(size='none')+
    labs(x=paste('Component 1 (',round(attr(projected[[1]],'explvar')[1]),'%)',sep=''),
         y=paste('Component 2 (',round(attr(projected[[1]],'explvar')[2]),'%)',sep=''),
         title='Score plot')+
    theme_bw()
}

plotscoreasca(model,k=2)
```

The ellipses that should characterize each level inside
the second feature are overlapping because, as said before, values are simulated and there is no real effect on the dependent variable.

At last, a statistical test for each variable can be carried out and the distribution of interest, obtained from permutation, can be compared with the observed test statistic that comes from the original dataset. As this kind of model has been developed for high-dimensional dataset, the rising number of dependent variables might imply that the test will take a few minutes. 

Because of that, in this implementation the loop that carries out the calculations is parallelized with the `doParallel`, in order to use all cores of the PC.

```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width='70%', fig.align='center'}
library(doParallel)
library(foreach)
# try parallelization
stat=c()
b=foreach(i = 1:1000, .combine = 'c') %do% {
  # reproducibillity
  seed=181299+i*10
  set.seed(seed)
  # permutation
  shuff=ymat[permute(1:dim(ymat)[1]),]
  # create dataset that will be used to test
  unl=unlist(shuff, use.names=F)
  baba=data.frame(Y=I(matrix(as.numeric(unl), ncol = dim(ymat)[2])),
                  doe)
  # full model
  mod.full=asca(Y~.,
                data = baba,
                pca.in=5)
  sse.full=as.data.frame(t(mod.full$ssq))$res
  df.full=length(mod.full$effects) -1
  # reduced model
  mod.reduced=asca(Y~ . - h,
                   data = baba,
                   pca.in=5)
  sse.reduced=as.data.frame(t(mod.reduced$ssq))$res
  df.reduced=length(mod.reduced$effects) -1
  # stat. test
  F=abs(((sse.reduced-sse.full)/( df.reduced - df.full))/(sse.full/df.full))
  # distribuzione
  stat=rbind(stat,F)
}
# calculate observed statistic
# full
mod.fin.full=asca(Y~.,
                  data = baba,
                  pca.in=2)
sse.full=as.data.frame(t(mod.fin.full$ssq))$res
df.full=length(mod.fin.full$effects) -1
# reduced
mod.fin.red=asca(Y~.-h,
                 data = baba,
                 pca.in=2)
sse.red=as.data.frame(t(mod.fin.red$ssq))$res
df.red=length(mod.fin.red$effects) -1
# statistic
F.fin=abs(((sse.red-sse.full)/( df.red - df.full))/(sse.full/df.full))
# after the testing loop has been computed (see testing.R),
# using the stat vector as input, it is possible to
# plot an histogram 
stat=as.data.frame(stat)
names(stat)='stat'
abline=as.data.frame(cbind(F.fin,quantile(stat$stat,0.95)))
ggplot(data=stat,aes(x=stat))+
  geom_histogram(color='black', fill='red', alpha=0.62, bins=30)+
  labs(y='Count',
       x='Permutation statistics',
       title='Distribution of the test statistic')+
  annotate(geom='text',x=c(quantile(stat$stat,0.95)+0.0005,F.fin-0.0005),y=c(125),
           label=c('95-th percentile','Observed F'),color='black', size=2.7)+
  geom_vline(xintercept=c(quantile(stat$stat,0.95),F.fin),
             color=c('red','blue'), linetype='dashed', linewidth=1, show.legend=T)+
  theme_bw()
```

As expected, the observed test statistic is far from being significant.


## Bibliography

* [ASCA in One Hour](https://www.youtube.com/watch?v=mySaqM7cZ5Q&t=2698s)

* [Statistical validation of megavariate effects in ASCA](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-322)

* [ASCA: analysis of multivariate data obtained from an experimental design](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.952?casa_token=5gnlC5Q4L64AAAAA:2TQANN-8FX7KYvo47lwiiKpQhmYYVHXltM494w3SzGyNgv2Bkhfs76UZ6B3CS31xzD4SbM5Pe2XnFM9j)